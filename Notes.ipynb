{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspak project \n",
    "# Macbook  or windows machine - python 3.10 is installed ( global version )\n",
    "\n",
    "# I created one project - retailproject ( used same global version of python 3.10 )\n",
    "# required packages -\n",
    "# pyspark 3.2.1\n",
    "# pytest ( some version )\n",
    "\n",
    "# Another project - lendingclub project \n",
    "# incase it requires -\n",
    "# python 3.8\n",
    "# pyspark 3.5\n",
    "# different version of pytest \n",
    "\n",
    "# so what to do in case need to use the global version of python installed \n",
    "# if so then need ot install another version of python - 3.8 , which will impact the other projects\n",
    "\n",
    "# so what is the solution ?\n",
    "# pipenv --> pip + venv \n",
    "\n",
    "# venv --> create a isolated virtual environment for your projects \n",
    "# five projects in your system:\n",
    "# virtual env 1 for project 1 --> python 3.6 , pyspark 3.2 etc.,\n",
    "# virtual env 2 for project 2 --> python 3.8 , pyspark 3.4 etc.,\n",
    "\n",
    "# pip --> way to install packages ( package manager )\n",
    "\n",
    "# each project should have own set of python , own set of packages ( few global installations are common )\n",
    "\n",
    "# old way \n",
    "# .venv \n",
    "# create virtual environmet \n",
    "# activate virtual environment \n",
    "# install packages required \n",
    "\n",
    "# new way \n",
    "# pip install pipenv            --> first need to run this as it requires pip to be present ( python need to installed globally to run this ) , it is package manager \n",
    "# pipenv install pytest         --> this creates the virtual envirionment and package also is installed , it considers global version of python , if different version is required then can change the Pipfile and do it \n",
    "# pipenv shell                  --> to activate the virtual environment \n",
    "# Pipfile                       --> similar to requirements.txt \n",
    "# pipfile.lock                  --> has the exact version of each packages with sha \n",
    "# exit                          -->to come out of the enviroment \n",
    "# pipenv run python             --> to run python without activating the environment \n",
    "# pipenv install pytest --dev   --> to create the package in development environment only \n",
    "# pipenv uninstall pytest       --> to uninstall pytest package\n",
    "#   \n",
    "# pipenv --rm                   --> to remove the environment , but the pipfile and pipfile.lock are still present \n",
    "# pipenv install                --> to create an enviroment from Pipfile ( but should have Pipfile in the path ) \n",
    "\n",
    "# if we need a specific version of python or package then in pipfile change it to specific version and run pipenv --rm to remove the environment and run pipenv install to install the new environment with required versions \n",
    "\n",
    "# to get the specific version of any packages can change in Pipfile ( \"==0.0.1\" for package version under the required package inplace of *)\n",
    "# can even change the python version in pipfile and install the package again ( pipenv --rm )\n",
    "# if the specifc version of python mentioned in Pipfile, if not found in system then it try to install  \n",
    "# Pyenv is used to install specific version of python ( pyenv install 3.9 , ./path/pyenv install 3.9 )\n",
    "\n",
    "# when setting up the test or production environment for the same project use pipfile.lock as it has the exact versions of python and python packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Structuring and execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Project Structuring and execution \n",
    "# Retail Analysis\n",
    "# Lets say we have :\n",
    "# Customers.csv ( customer_id ,customer_fname,customer_lname,username ,password , address, city , state , pincode  )\n",
    "# orders.csv ( order_id , order_date , customer_id , order_status )\n",
    "# Problem statement -  Need to find the number of closed orders for each state \n",
    "\n",
    "# steps:\n",
    "# create orders dataframe\n",
    "# create cutsomers dataframe\n",
    "# filter the orders dataframe \n",
    "# join the filtered orders dataframe with customers dataframe \n",
    "# group by on the state to get the count \\\n",
    "\n",
    "# to implement these steps in a notebook it is easy and simple \n",
    "\n",
    "# but how to do this in structured format , so that the code is modular and follows industry standards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_of_project\n",
    "#     - configs ( to keep the configurations )\n",
    "#         - application.conf ( to keep the configs reated to the project like file_path etc.,)\n",
    "#         - py_spark.conf ( to keep spark level configs like executor core ,executor memory , application name etc.,)\n",
    "#     - lib ( to create the reusable libraries ) \n",
    "#         - configreader.py ( to read the configs created under configs files )\n",
    "#         - datareader.py ( create the spark dataframe from data files )\n",
    "#         - datamanipulation.py ( all the data manipulations , transformations ) \n",
    "#         - utils.py ( any utlility functions like creating spark session, creating any folders etc.,)\n",
    "#     - data ( to keep the sample data for tesing , ideally data will be in datalake ) \n",
    "#         - orders.csv\n",
    "#         - customers.csv \n",
    "#     - application_main.py ( entry point for the spark application , where all the functions defined are called ) \n",
    "#     - pipfile \n",
    "#     - pipfile.lock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from pyspark import SparkConf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"configs/application.conf\")\n",
    "app_conf = {}\n",
    "# for (key, val) in config.items(env):\n",
    "#     app_conf[key] = val\n",
    "#     app_conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x1edd019deb0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"configs/pyspark.conf\")\n",
    "pyspark_conf = SparkConf()\n",
    "\n",
    "for (key, val) in config.items(\"TEST\"):\n",
    "    pyspark_conf.set(key, val)\n",
    "pyspark_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'retail-test'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#config.items(\"LOCAL\")\n",
    "pyspark_conf.get(\"spark.app.name\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('customers.file.path', 'data/customers.csv'),\n",
       " ('orders.file.path', 'data/orders.csv')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.items(\"LOCAL\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit testing \n",
    "\n",
    "# Good developers write unit test cases \n",
    "# testing small pieces of code to ensure it is working fine \n",
    "# testing samll unit of code \n",
    "# If the code is written in modular way then testing of each function seperately \n",
    "# one unit == one function \n",
    "\n",
    "# many frameworks to perform unit testing \n",
    "# unittest\n",
    "# pytest \n",
    "\n",
    "# need to write test cases as a part off best practices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which units or functions of the codes need to be tested \n",
    "\n",
    "# read_customers         --> if the output is 12435 then it is passed \n",
    "# read_orders            --> if the output is 68884 then it is passed \n",
    "# filter_closed_orders   --> if the count is 7556 then it is passed \n",
    "# get_app_config         --> test if the configurations are read properly \n",
    "\n",
    "# how to test these functions ?\n",
    "# need to create a file where one can write the unit tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in pytest framework file name where we write the unit test cases should either start with test or end with test \n",
    "# test_retail_proj.py\n",
    "# retail_proj_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m pytest is the command to run testing \n",
    "# python -m pytest -v "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial set of functions \n",
    "\n",
    "# import pytest \n",
    "# from lib.utils import get_spark_session\n",
    "# from lib.datareader import read_customers, read_orders \n",
    "# from lib.datamanipulation import filter_closed_orders \n",
    "# from lib.configreader import get_app_config \n",
    "\n",
    "# def test_read_customers(): # function should start with test    \n",
    "#     spark_s=get_spark_session(\"LOCAL\")\n",
    "#     customers_count=read_customers(spark_s,\"LOCAL\").count()\n",
    "#     assert customers_count == 12435 # assert to check the true condition \n",
    "\n",
    "# def test_read_orders(): # function should start with test  \n",
    "#     spark_s=get_spark_session(\"LOCAL\")\n",
    "#     orders_count=read_orders(spark_s,\"LOCAL\").count()\n",
    "#     assert orders_count == 68884 # checking on the number of records \n",
    "\n",
    "# def test_filtered_orders(): # function should start with test    \n",
    "#     spark_s=get_spark_session(\"LOCAL\")\n",
    "#     df_=filter_closed_orders(read_orders(spark_s,\"LOCAL\")).count()\n",
    "#     assert df_ == 7556  \n",
    "\n",
    "# def test_app_config():\n",
    "#     dict_value_returned_=get_app_config(\"LOCAL\")\n",
    "#     assert dict_value_returned_[\"customers.file.path\"] == \"data/customers.csv\" # testing any one value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the inital set of functions can see spark session creation is a part of function testing  \n",
    "# which is not expected \n",
    "# these has to be under fixtures \n",
    "\n",
    "# fixture is to write the setup code \n",
    "# write fixtures in conftest.py\n",
    "\n",
    "# fixtures are picked automatically from conftest.py files  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup - fixture \n",
    "# perform unit testing - define unit testing \n",
    "# teardown - releasing the resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m pytest --fixtures # to get the list of fixtures \n",
    "# few fixtures will be system defined and few will be user defined "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixtures to check if calculated results match the expected results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixtures to check if calculated results match the expected results \n",
    "\n",
    "# now need to create another test case \n",
    "# need to test function  \"count_orders_state\"\n",
    "# to test this the pre calculated results are already calcuated and kept in the data > test_results folder \n",
    "# now need ot create the function to test in test_retail_proj.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"data/customers.csv\").groupby([\"state\"])[\"customer_id\"].count().reset_index().to_csv(\"data/test_results/state_aggregate.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markers \n",
    "# to label test cases use markers\n",
    "# if there are hundred test cases out of which forty are for dataloading testing can maker them using markers \n",
    "# using\n",
    "# eg: @pytest.mark.dataloading\n",
    "# python -m pytest -m datamanipulation # to run only the test cases marked as datamanipulation test cases \n",
    "\n",
    "# python -m pytest --markers # to get the list of markers, including system defined and user defined \n",
    "\n",
    "# pytest.ini # to mention the markers so that it can be used as pytest configuration \n",
    "# [pytest]\n",
    "# markers =\n",
    "#         datareading: test the data reading functions\n",
    "#         datamanipulation: tests the data manipulation functions\n",
    "\n",
    "# python -m pytest -m \"not datamanipulation\"  # to select and execute the test cases apart from datamanipulation   \n",
    "\n",
    "# if any of the test case is not fully developed and work in progress\n",
    "# the can mention \n",
    "# @pytest.mark.skip(\"work in progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterised generic test cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameterised generic test cases \n",
    "# In function \"filter_closed_orders(orders_df)\" the filtering is hardcoded to \"CLOSED\" , instead of this need to pass the generic condition . i.e passing the parameter to function \n",
    "# defined as function \"filter_closed_orders_parameterised\" in datamanipulation.py by passign the status parameter \n",
    "\n",
    "# in test_retail_proj.py can create multiple functions for closed , complete , processing etc., but in this approcah the lot of redundecy is present \n",
    "# can see functions created in test_filter_closed_orders_generic, test_filter_complete_orders_generic , test_filter_pending_orders_generic etc., created in test_retail_proj.py\n",
    "\n",
    "# so make is it more efficient \n",
    "# need to define the parameters to pass using the below in test_retail_proj.py :\n",
    "\n",
    "# @pytest.mark.parametrize(\n",
    "#         \"status_,count_\",\n",
    "#         [(\"COMPLETE\",22900),\n",
    "#          (\"PENDING\",7610),\n",
    "#          (\"PROCESSING\",8275)]\n",
    "# )\n",
    "\n",
    "# which will run the text case \"test_filter_orders_generic\" in test_reatil_proj.py by passing each parameter once \n",
    "# it passes the parameters to,  the test case or functions where the above parameters are defined \n",
    "\n",
    "# for good production application writing test cases is very good practices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logging \n",
    "#### all logging related files are in \"C:/Users/user/Downloads/Ultimate_bigdata/Week13/Project_Structuring_pytest_materials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging \n",
    "# logging is required when an application is productionised  \n",
    "# getting the logs \n",
    "\n",
    "# to understand the flow of control used print statements and for debuggging \n",
    "# few issues with print for debuggimg \n",
    "# 1.cannot set the priorities logging level like info, warn, error ,fatal etc.,\n",
    "# 2.if there are multiple print statements are used durimg debugging , then manual efforts are required to removes\n",
    "# 3. print makes aplications slow  \n",
    "\n",
    "# logging is the best way to solve the issues   \n",
    "# Log4j is used for logging \n",
    "# spark internally uses log4j for logging , so we can reuse the same for application logs \n",
    "\n",
    "# instance of log4j object can be obtained from spark session \n",
    "\n",
    "# in utils.py adding extra config while creating the spark session \n",
    "# new files gets created -- log4j.properties and logger.py\n",
    "# modification -- application_main.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging levels :\n",
    "# debug - verbose related information\n",
    "# info - all information related things \n",
    "# warn - warning messages \n",
    "# error - error related messages\n",
    "# fatal - bigger error \n",
    "\n",
    "# in log4j.properties file if loggimg level is defined as debug all debug , info , warn , error and fatal messages are displayed \n",
    "# in log4j.properties file if loggimg level is defined as info  all  info , warn , error and fatal messages are displayed \n",
    "# in the same way if warn --> warn , error , fatal are displayed \n",
    "# error --> error , fatal\n",
    "# fatal --> fatal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target location \n",
    "# target location to log\n",
    "# 1. console \n",
    "# 2. file \n",
    "\n",
    "# message format \n",
    "# message format to display the log \n",
    "\n",
    "# logging level , target location and message format are main things to consider "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log4j.properties file ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log4j.properties file , is placed in root \n",
    "\n",
    "# # root\n",
    "# log4j.rootCategory=INFO, console   --> INFO - logging level , console - to print \n",
    "\n",
    "# # console appender\n",
    "# log4j.appender.console=org.apache.log4j.ConsoleAppender  --> console appender , to console  ( incase of file as traget need to have FileAppender and respecive properties , if two appenders are defined then will all in both places )\n",
    "# log4j.appender.console.target=System.out\n",
    "# log4j.appender.console.layout=org.apache.log4j.PatternLayout --> pattern layout to print the log\n",
    "# log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n  --> print log in specific format \n",
    "#                                                 # %d -- date , %p -- priority , %c -- script name / caller , %m -- message , %n -- newlines  \n",
    "\n",
    "# # spark configs\n",
    "# log4j.logger.org.apache.spark.repl.Main=WARN\n",
    "# log4j.logger.org.spark_project.jetty=WARN\n",
    "# log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle=ERROR\n",
    "# log4j.logger.org.apache.parquet=ERROR\n",
    "# log4j.logger.parquet=ERROR\n",
    "# log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL\n",
    "# log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR\n",
    "# log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO\n",
    "# log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO\n",
    "\n",
    "# #user logs\n",
    "# log4j.logger.retail_analysis=ERROR, console # for the respective application , retail_analysis is the project folder names\n",
    "# log4j.additivity.retail_analysis=false  # to avoid double logging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logger.py   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.py \n",
    "\n",
    "# under lib logger.py is placed \n",
    "# the code inside it is boiler plate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logging main script    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging main script \n",
    "\n",
    "# spark = Utils.get_spark_session(job_run_env)          # creating spark session \n",
    "\n",
    "# logger = Log4j(spark) # log4j instances \n",
    "\n",
    "# logger.warn(\"Created Spark Session\")                  # logging  \n",
    "\n",
    "# orders_df = DataReader.read_orders(spark,job_run_env)\n",
    "\n",
    "# orders_filtered = DataManipulation.filter_closed_orders(orders_df)\n",
    "\n",
    "# customers_df = DataReader.read_customers(spark,job_run_env)\n",
    "\n",
    "# joined_df = DataManipulation.join_orders_customers(orders_filtered,customers_df)\n",
    "\n",
    "# aggregated_results = DataManipulation.count_orders_state(joined_df)\n",
    "\n",
    "# aggregated_results.show(50)\n",
    "\n",
    "# #print(aggregated_results.collect())\n",
    "\n",
    "# logger.info(\"this is the end of main\")                   # logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CANCELED</td>\n",
       "      <td>1428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CLOSED</td>\n",
       "      <td>7556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>22900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ON_HOLD</td>\n",
       "      <td>3798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PAYMENT_REVIEW</td>\n",
       "      <td>729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PENDING</td>\n",
       "      <td>7610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PENDING_PAYMENT</td>\n",
       "      <td>15030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PROCESSING</td>\n",
       "      <td>8275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SUSPECTED_FRAUD</td>\n",
       "      <td>1558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      order_status  order_id\n",
       "0         CANCELED      1428\n",
       "1           CLOSED      7556\n",
       "2         COMPLETE     22900\n",
       "3          ON_HOLD      3798\n",
       "4   PAYMENT_REVIEW       729\n",
       "5          PENDING      7610\n",
       "6  PENDING_PAYMENT     15030\n",
       "7       PROCESSING      8275\n",
       "8  SUSPECTED_FRAUD      1558"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dff_=pd.read_csv(\"data/orders.csv\")\n",
    "dff_.groupby([\"order_status\"])[\"order_id\"].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git commands\n",
    "# git.scm -- to install the git ( local )\n",
    "# \n",
    "# git --version\n",
    "# In Windows git bash is used to wrtie commands \n",
    "# \n",
    "# project is developed using IDE like VScode , Pycharm etc.,\n",
    "# github.com is used to create the github ( remotes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb6d8c538dbe025f231499341cd39fcddafd81b87c0773ec5727e09f7e3ddf49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
